# ğŸ›ï¸ ë¯¼ë²• ë¶€ë™ì‚° ë²•ë¥  ìƒë‹´ ì±—ë´‡ (GPT2 Fine-tuning)

í•œêµ­ì–´ GPT2 ëª¨ë¸ì„ í™œìš©í•œ ë¯¼ë²• ë¶€ë™ì‚° ë° ì „ì„¸ ê´€ë ¨ ë²•ë¥  ìƒë‹´ ì±—ë´‡ì…ë‹ˆë‹¤. LoRA(Low-Rank Adaptation)ë¥¼ ì‚¬ìš©í•˜ì—¬ íš¨ìœ¨ì ìœ¼ë¡œ fine-tuningí•˜ë©°, CPU ë° GPU í™˜ê²½ ëª¨ë‘ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

- [í”„ë¡œì íŠ¸ ê°œìš”](#-í”„ë¡œì íŠ¸-ê°œìš”)
- [ì£¼ìš” íŠ¹ì§•](#-ì£¼ìš”-íŠ¹ì§•)
- [ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­](#-ì‹œìŠ¤í…œ-ìš”êµ¬ì‚¬í•­)
- [ì„¤ì¹˜ ë°©ë²•](#-ì„¤ì¹˜-ë°©ë²•)
- [í”„ë¡œì íŠ¸ êµ¬ì¡°](#-í”„ë¡œì íŠ¸-êµ¬ì¡°)
- [ì‚¬ìš© ë°©ë²•](#-ì‚¬ìš©-ë°©ë²•)
- [ì„¤ì • íŒŒì¼ ì„¤ëª…](#-ì„¤ì •-íŒŒì¼-ì„¤ëª…)
- [ë°ì´í„°ì…‹ í˜•ì‹](#-ë°ì´í„°ì…‹-í˜•ì‹)
- [íŠ¸ëŸ¬ë¸”ìŠˆíŒ…](#-íŠ¸ëŸ¬ë¸”ìŠˆíŒ…)
- [ë¼ì´ì„ ìŠ¤](#-ë¼ì´ì„ ìŠ¤)

## ğŸ¯ í”„ë¡œì íŠ¸ ê°œìš”

ì´ í”„ë¡œì íŠ¸ëŠ” í•œêµ­ ë¯¼ë²•, íŠ¹íˆ ë¶€ë™ì‚° ë° ì „ì„¸ ê´€ë ¨ ë²•ë¥  ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AI ì±—ë´‡ì„ êµ¬ì¶•í•©ë‹ˆë‹¤. **Llama ëª¨ë¸ì—ì„œ GPT2 ëª¨ë¸ë¡œ ë³€ê²½**í•˜ì—¬ ë” ê°€ë²¼ìš´ í™˜ê²½ì—ì„œë„ ì‹¤í–‰ ê°€ëŠ¥í•˜ë„ë¡ ìµœì í™”í–ˆìŠµë‹ˆë‹¤.

### ì£¼ìš” ë³€ê²½ì‚¬í•­ (Llama â†’ GPT2)

- **ëª¨ë¸**: Meta Llama â†’ skt/kogpt2-base-v2
- **ì•„í‚¤í…ì²˜**: Llama â†’ GPT2
- **íƒ€ê²Ÿ ëª¨ë“ˆ**: `q_proj`, `v_proj` â†’ `c_attn`, `c_proj`
- **í”„ë¡¬í”„íŠ¸ í˜•ì‹**: ë³µì¡í•œ chat template â†’ ê°„ë‹¨í•œ Q&A í˜•ì‹
- **í† í° ì²˜ë¦¬**: íŠ¹ìˆ˜ í† í° ê°„ì†Œí™”

## âœ¨ ì£¼ìš” íŠ¹ì§•

- âœ… **ê²½ëŸ‰ ëª¨ë¸**: skt/kogpt2-base-v2 ì‚¬ìš©ìœ¼ë¡œ ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„± í–¥ìƒ
- âœ… **LoRA Fine-tuning**: ì „ì²´ ëª¨ë¸ ì¬í•™ìŠµ ëŒ€ì‹  ì¼ë¶€ íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµ
- âœ… **CPU/GPU ì§€ì›**: CPU ì „ìš© í™˜ê²½ì—ì„œë„ í›ˆë ¨ ë° ì¶”ë¡  ê°€ëŠ¥
- âœ… **ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤**: ì‹¤ì‹œê°„ ì§ˆì˜ì‘ë‹µ ê¸°ëŠ¥
- âœ… **ë©”ëª¨ë¦¬ ìµœì í™”**: 8bit ì–‘ìí™” ë° ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ì§€ì›
- âœ… **ëª¨ë“ˆí™”ëœ êµ¬ì¡°**: ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ì»´í¬ë„ŒíŠ¸ ì„¤ê³„

## ğŸ’» ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

### GPU í™˜ê²½
- **GPU**: NVIDIA RTX 4060 8GB ì´ìƒ
- **RAM**: 16GB ì´ìƒ
- **ë””ìŠ¤í¬**: 20GB ì´ìƒ ì—¬ìœ  ê³µê°„
- **CUDA**: 12.6 ì´ìƒ
- **Python**: 3.9 ì´ìƒ

### CPU í™˜ê²½
- **CPU**: ë©€í‹°ì½”ì–´ í”„ë¡œì„¸ì„œ (4ì½”ì–´ ì´ìƒ ê¶Œì¥)
- **RAM**: 16GB ì´ìƒ (32GB ê¶Œì¥)
- **ë””ìŠ¤í¬**: 20GB ì´ìƒ ì—¬ìœ  ê³µê°„
- **Python**: 3.9

âš ï¸ **CPU í™˜ê²½ ì£¼ì˜ì‚¬í•­**: CPUì—ì„œëŠ” í›ˆë ¨ ì†ë„ê°€ ë§¤ìš° ëŠë¦½ë‹ˆë‹¤ (GPU ëŒ€ë¹„ 10~50ë°°). í…ŒìŠ¤íŠ¸ ë° ì¶”ë¡  ìš©ë„ë¡œ ê¶Œì¥í•©ë‹ˆë‹¤.

## ğŸ“¦ ì„¤ì¹˜ ë°©ë²•

### 1. ì €ì¥ì†Œ í´ë¡ 

```bash
git clone https://github.com/duck3244/fine_tuning_cpu_base_model.git
```

### 2. ê°€ìƒí™˜ê²½ ìƒì„± (ê¶Œì¥)

```bash
# Windows
python -m venv venv
venv\Scripts\activate

 - Miniconda 3 ê¸°ì¤€
  -- C:\Users\User\miniconda3>cd Scripts
  -- conda create -n py39_tf python=3.9
  -- conda activate py39_tf

# Linux/Mac
python3 -m venv venv
source venv/bin/activate
```

### 3. íŒ¨í‚¤ì§€ ì„¤ì¹˜

#### GPU í™˜ê²½

```bash
# PyTorch GPU ë²„ì „ ì„¤ì¹˜
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# ê¸°íƒ€ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements.txt
```

#### CPU í™˜ê²½ (Windows) - ê¶Œì¥

```bash
# PyTorch CPU ë²„ì „ ì„¤ì¹˜
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu

# CPU ì „ìš© íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements_cpu_py39.txt
```

### 4. ì„¤ì¹˜ í™•ì¸

```bash
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}')"
```

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
llama_finetune_project/
â”œâ”€â”€ ğŸ“„ config_cpu.py              # CPU ì „ìš© ì„¤ì • íŒŒì¼ (GPT2)
â”œâ”€â”€ ğŸ“„ data_loader.py             # ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬
â”œâ”€â”€ ğŸ“„ model_manager_cpu.py       # CPU ì „ìš© ëª¨ë¸ ê´€ë¦¬
â”œâ”€â”€ ğŸ“„ trainer.py                 # í›ˆë ¨ ê´€ë ¨ ìœ í‹¸ë¦¬í‹°
â”œâ”€â”€ ğŸ“„ inference_manager.py       # ì¶”ë¡  ë° í…ìŠ¤íŠ¸ ìƒì„±
â”œâ”€â”€ ğŸ“„ utils.py                   # ê³µí†µ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
â”œâ”€â”€ ğŸ“„ main_train_cpu.py          # CPU ì „ìš© í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ ğŸ“„ main_inference_cpu.py      # CPU ì „ìš© ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ ğŸ“„ requirements.txt           # GPU íŒ¨í‚¤ì§€ ì˜ì¡´ì„±
â”œâ”€â”€ ğŸ“„ requirements_cpu_py39.txt  # CPU íŒ¨í‚¤ì§€ ì˜ì¡´ì„± (Python 3.9)
â”œâ”€â”€ ğŸ“„ civil_law_qa_dataset.csv   # Q/A ë°ì´í„°ì…‹
â”œâ”€â”€ ğŸ“„ README.md                  # í”„ë¡œì íŠ¸ ì„¤ëª…ì„œ
â””â”€â”€ ğŸ“ fine_tuned_model_cpu/      # í›ˆë ¨ëœ ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬
```

## ğŸš€ ì‚¬ìš© ë°©ë²•

### 1. ë°ì´í„° ì¤€ë¹„

`civil_law_qa_dataset.csv` íŒŒì¼ì´ í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.

```bash
# CSV íŒŒì¼ ê²€ì¦
python -c "from utils import validate_csv_file; validate_csv_file('civil_law_qa_dataset.csv')"
```

### 2. ëª¨ë¸ í›ˆë ¨

#### CPU í™˜ê²½ì—ì„œ í›ˆë ¨

```bash
python main_train_cpu.py \
    --csv_path civil_law_qa_dataset.csv \
    --output_dir ./fine_tuned_model_cpu \
    --max_steps 100 \
    --batch_size 1 \
    --learning_rate 5e-5
```

#### ì£¼ìš” í›ˆë ¨ íŒŒë¼ë¯¸í„°

- `--csv_path`: ë°ì´í„°ì…‹ CSV íŒŒì¼ ê²½ë¡œ
- `--output_dir`: ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬
- `--max_steps`: ìµœëŒ€ í›ˆë ¨ ìŠ¤í… ìˆ˜ (CPUëŠ” 100 ê¶Œì¥)
- `--batch_size`: ë°°ì¹˜ í¬ê¸° (CPUëŠ” 1 ê¶Œì¥)
- `--epochs`: ì—í­ ìˆ˜ (CPUëŠ” 1 ê¶Œì¥)
- `--learning_rate`: í•™ìŠµë¥  (ê¸°ë³¸ê°’: 5e-5)

#### ì„¤ì • ê²€ì¦ë§Œ ì‹¤í–‰ (Dry Run)

```bash
python main_train_cpu.py --dry_run
```

### 3. ëª¨ë¸ ì¶”ë¡ 

#### ëŒ€í™”í˜• ëª¨ë“œ

```bash
python main_inference_cpu.py \
    --model_path ./fine_tuned_model_cpu \
    --interactive
```

#### ë‹¨ì¼ ì§ˆë¬¸

```bash
python main_inference_cpu.py \
    --model_path ./fine_tuned_model_cpu \
    --question "ì „ì„¸ê¶Œì´ë€ ë¬´ì—‡ì¸ê°€ìš”?"
```

#### ë¹ ë¥¸ í…ŒìŠ¤íŠ¸

```bash
python main_inference_cpu.py \
    --model_path ./fine_tuned_model_cpu \
    --quick_test
```

#### ì£¼ìš” ì¶”ë¡  íŒŒë¼ë¯¸í„°

- `--model_path`: Fine-tuned ëª¨ë¸ ê²½ë¡œ
- `--interactive`: ëŒ€í™”í˜• ëª¨ë“œ ì‹¤í–‰
- `--question`: ë‹¨ì¼ ì§ˆë¬¸ ì…ë ¥
- `--quick_test`: ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
- `--max_new_tokens`: ìµœëŒ€ ìƒì„± í† í° ìˆ˜ (ê¸°ë³¸ê°’: 256)
- `--temperature`: ìƒì„± ì˜¨ë„ (ê¸°ë³¸ê°’: 0.8)

## âš™ï¸ ì„¤ì • íŒŒì¼ ì„¤ëª…

### config_cpu.py

GPT2 ëª¨ë¸ìš© CPU ìµœì í™” ì„¤ì • íŒŒì¼ì…ë‹ˆë‹¤.

```python
# ëª¨ë¸ ì„¤ì •
model_name = "skt/kogpt2-base-v2"  # í•œêµ­ì–´ GPT2
max_length = 256                    # ì‹œí€€ìŠ¤ ê¸¸ì´
torch_dtype = torch.float32         # CPUëŠ” float32

# LoRA ì„¤ì •
r = 8                               # LoRA rank (ê°ì†Œ)
lora_alpha = 16                     # ìŠ¤ì¼€ì¼ë§ íŒ©í„°
target_modules = ["c_attn", "c_proj"]  # GPT2 íƒ€ê²Ÿ ëª¨ë“ˆ

# í›ˆë ¨ ì„¤ì •
batch_size = 1                      # ìµœì†Œ ë°°ì¹˜
gradient_accumulation_steps = 32    # íš¨ê³¼ì  ë°°ì¹˜ = 32
max_steps = 100                     # CPU ìµœì í™”
```

### í”„ë¡¬í”„íŠ¸ í˜•ì‹ ë³€ê²½

**ì´ì „ (Llama)**:
```
<|begin_of_text|><|start_header_id|>system<|end_header_id|>
ë‹¹ì‹ ì€ í•œêµ­ ë¯¼ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
<|start_header_id|>user<|end_header_id|>
ì§ˆë¬¸ë‚´ìš©
<|start_header_id|>assistant<|end_header_id|>
ë‹µë³€ë‚´ìš©
```

**í˜„ì¬ (GPT2)**:
```
ì§ˆë¬¸: ì§ˆë¬¸ë‚´ìš©
ë‹µë³€: ë‹µë³€ë‚´ìš©</s>
```

## ğŸ“Š ë°ì´í„°ì…‹ í˜•ì‹

CSV íŒŒì¼ì€ ë‹¤ìŒ ì»¬ëŸ¼ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤:

| ì»¬ëŸ¼ | í•„ìˆ˜ | ì„¤ëª… |
|------|------|------|
| question | âœ… | ì§ˆë¬¸ ë‚´ìš© |
| answer | âœ… | ë‹µë³€ ë‚´ìš© |
| category | âŒ | ì¹´í…Œê³ ë¦¬ (ì„ íƒ) |
| difficulty | âŒ | ë‚œì´ë„ (ì„ íƒ) |

### ì˜ˆì‹œ

```csv
question,answer,category,difficulty
"ì „ì„¸ê¶Œì´ë€ ë¬´ì—‡ì¸ê°€ìš”?","ì „ì„¸ê¶Œì€ ì „ì„¸ê¸ˆì„ ì§€ê¸‰í•˜ê³  íƒ€ì¸ì˜ ë¶€ë™ì‚°ì„ ì ìœ í•˜ì—¬ ê·¸ ë¶€ë™ì‚°ì˜ ìš©ë„ì— ë§ê²Œ ì‚¬ìš©Â·ìˆ˜ìµí•˜ë©°, í›„ì— ê·¸ ë¶€ë™ì‚° ì „ë¶€ì— ëŒ€í•˜ì—¬ í›„ìˆœìœ„ê¶Œë¦¬ì ê¸°íƒ€ ì±„ê¶Œìë³´ë‹¤ ì „ì„¸ê¸ˆì˜ ìš°ì„ ë³€ì œë¥¼ ë°›ì„ ìˆ˜ ìˆëŠ” ê¶Œë¦¬ì…ë‹ˆë‹¤.",ë¶€ë™ì‚°ë¬¼ê¶Œ,ì´ˆê¸‰
```

## ğŸ”§ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 1. GPU ë©”ëª¨ë¦¬ ë¶€ì¡±

```bash
# config_cpu.pyì—ì„œ ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
batch_size = 1
gradient_accumulation_steps = 32
```

### 2. CPU í›ˆë ¨ì´ ë„ˆë¬´ ëŠë¦¼

```bash
# max_steps ì œí•œ
python main_train_cpu.py --max_steps 50
```

### 3. ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŒ ì˜¤ë¥˜

```bash
# íŒ¨í‚¤ì§€ ì¬ì„¤ì¹˜
pip install --upgrade -r requirements_cpu_py39.txt
```

### 4. í† í¬ë‚˜ì´ì € ì˜¤ë¥˜

GPT2 ëª¨ë¸ì€ íŠ¹ë³„í•œ í† í° ì„¤ì •ì´ ê°„ë‹¨í•©ë‹ˆë‹¤:
- `pad_token = eos_token`
- `bos_token = eos_token`

### 5. ì¶”ë¡  ê²°ê³¼ê°€ ì´ìƒí•¨

```bash
# ìƒì„± íŒŒë¼ë¯¸í„° ì¡°ì •
python main_inference_cpu.py \
    --temperature 0.7 \
    --max_new_tokens 512
```

## ğŸ“ˆ ì„±ëŠ¥ ìµœì í™” íŒ

### CPU í™˜ê²½

1. **ìŠ¤ë ˆë“œ ìˆ˜ ìµœì í™”**: `config_cpu.py`ì—ì„œ ìë™ ì„¤ì •ë¨
2. **ë°°ì¹˜ í¬ê¸° ìµœì†Œí™”**: `batch_size=1`
3. **ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì **: `gradient_accumulation_steps=32`
4. **ìŠ¤í… ìˆ˜ ì œí•œ**: `max_steps=100`

### ë©”ëª¨ë¦¬ ì ˆì•½

1. **ì‹œí€€ìŠ¤ ê¸¸ì´ ê°ì†Œ**: `max_length=256`
2. **LoRA rank ê°ì†Œ**: `r=8`
3. **ë¶ˆí•„ìš”í•œ ë¡œê·¸ ì œê±°**: `report_to=None`

## ğŸ“ ëª¨ë¸ ì •ë³´

### GPT2 vs Llama ë¹„êµ

| íŠ¹ì§• | GPT2 | Llama |
|------|------|-------|
| ëª¨ë¸ í¬ê¸° | ~500MB | ~13GB |
| ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰ | ë‚®ìŒ | ë†’ìŒ |
| í›ˆë ¨ ì†ë„ | ë¹ ë¦„ | ëŠë¦¼ |
| í•œêµ­ì–´ ì„±ëŠ¥ | ìš°ìˆ˜ (skt/kogpt2) | ìš°ìˆ˜ |
| ì¶”ë¡  ì†ë„ | ë§¤ìš° ë¹ ë¦„ | ëŠë¦¼ |

### LoRA íŒŒë¼ë¯¸í„°

- **rank (r)**: 8 (Llama: 16)
- **alpha**: 16 (Llama: 32)
- **íƒ€ê²Ÿ ëª¨ë“ˆ**: `c_attn`, `c_proj` (GPT2 attention ë ˆì´ì–´)

## ğŸ“ ë¡œê·¸ íŒŒì¼

í›ˆë ¨ ë° ì¶”ë¡  ì‹œ ìë™ìœ¼ë¡œ ë¡œê·¸ íŒŒì¼ì´ ìƒì„±ë©ë‹ˆë‹¤:

- `training_cpu_YYYYMMDD_HHMMSS.log`
- `inference_cpu_YYYYMMDD_HHMMSS.log`

---
