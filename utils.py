"""
Utility functions for the project
"""

import os
import json
import logging
import subprocess
import torch
from typing import Dict, List, Optional, Any
from datetime import datetime
import psutil

logger = logging.getLogger(__name__)

def setup_logging(log_level: str = "INFO", log_file: Optional[str] = None):
    """ë¡œê¹… ì„¤ì •"""
    log_format = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    
    # ë¡œê·¸ ë ˆë²¨ ì„¤ì •
    level = getattr(logging, log_level.upper())
    
    # í•¸ë“¤ëŸ¬ ì„¤ì •
    handlers = [logging.StreamHandler()]
    
    if log_file:
        handlers.append(logging.FileHandler(log_file, encoding='utf-8'))
    
    # ê¸°ë³¸ ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=level,
        format=log_format,
        handlers=handlers,
        force=True
    )
    
    # transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œê·¸ ë ˆë²¨ ì¡°ì •
    logging.getLogger("transformers").setLevel(logging.WARNING)
    logging.getLogger("datasets").setLevel(logging.WARNING)
    logging.getLogger("tokenizers").setLevel(logging.WARNING)
    
    logger.info(f"Logging setup completed with level: {log_level}")

def check_system_requirements():
    """ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸"""
    logger.info("Checking system requirements...")
    
    # GPU í™•ì¸
    if not torch.cuda.is_available():
        logger.warning("CUDA is not available. Training will be very slow on CPU.")
        return False
    
    # GPU ë©”ëª¨ë¦¬ í™•ì¸
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
    logger.info(f"GPU Memory: {gpu_memory:.1f} GB")
    
    if gpu_memory < 6:
        logger.warning("GPU memory is less than 6GB. Training may fail.")
        return False
    
    # RAM í™•ì¸
    ram_gb = psutil.virtual_memory().total / 1024**3
    logger.info(f"System RAM: {ram_gb:.1f} GB")
    
    if ram_gb < 12:
        logger.warning("System RAM is less than 12GB. Consider closing other applications.")
    
    # ë””ìŠ¤í¬ ê³µê°„ í™•ì¸
    disk_usage = psutil.disk_usage('.')
    free_gb = disk_usage.free / 1024**3
    logger.info(f"Free disk space: {free_gb:.1f} GB")
    
    if free_gb < 10:
        logger.warning("Free disk space is less than 10GB. Model saving may fail.")
        return False
    
    logger.info("System requirements check passed!")
    return True

def get_gpu_info() -> Dict[str, Any]:
    """GPU ì •ë³´ ë°˜í™˜"""
    if not torch.cuda.is_available():
        return {"available": False}
    
    gpu_info = {
        "available": True,
        "device_count": torch.cuda.device_count(),
        "devices": []
    }
    
    for i in range(torch.cuda.device_count()):
        device_props = torch.cuda.get_device_properties(i)
        memory_allocated = torch.cuda.memory_allocated(i) / 1024**3
        memory_reserved = torch.cuda.memory_reserved(i) / 1024**3
        memory_total = device_props.total_memory / 1024**3
        
        device_info = {
            "id": i,
            "name": device_props.name,
            "total_memory_gb": memory_total,
            "allocated_memory_gb": memory_allocated,
            "reserved_memory_gb": memory_reserved,
            "free_memory_gb": memory_total - memory_reserved,
            "compute_capability": f"{device_props.major}.{device_props.minor}"
        }
        
        gpu_info["devices"].append(device_info)
    
    return gpu_info

def print_gpu_info():
    """GPU ì •ë³´ ì¶œë ¥"""
    gpu_info = get_gpu_info()
    
    if not gpu_info["available"]:
        print("âŒ CUDA not available")
        return
    
    print(f"\nğŸ® GPU Information:")
    print(f"   Device count: {gpu_info['device_count']}")
    
    for device in gpu_info["devices"]:
        print(f"\n   GPU {device['id']}: {device['name']}")
        print(f"   - Total Memory: {device['total_memory_gb']:.1f} GB")
        print(f"   - Free Memory: {device['free_memory_gb']:.1f} GB")
        print(f"   - Compute Capability: {device['compute_capability']}")

def clear_gpu_memory():
    """GPU ë©”ëª¨ë¦¬ ì •ë¦¬"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        logger.info("GPU memory cleared")

def set_environment_variables():
    """í™˜ê²½ ë³€ìˆ˜ ì„¤ì •"""
    env_vars = {
        "CUDA_VISIBLE_DEVICES": "0",
        "PYTORCH_CUDA_ALLOC_CONF": "max_split_size_mb:512",
        "TOKENIZERS_PARALLELISM": "false",
        "TRANSFORMERS_VERBOSITY": "warning"
    }
    
    for key, value in env_vars.items():
        os.environ[key] = value
        logger.info(f"Set {key}={value}")

def create_directory(path: str):
    """ë””ë ‰í† ë¦¬ ìƒì„±"""
    os.makedirs(path, exist_ok=True)
    logger.info(f"Directory created/verified: {path}")

def save_config_to_file(config_dict: Dict, filepath: str):
    """ì„¤ì •ì„ JSON íŒŒì¼ë¡œ ì €ì¥"""
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(config_dict, f, indent=2, ensure_ascii=False, default=str)
    logger.info(f"Config saved to {filepath}")

def load_config_from_file(filepath: str) -> Dict:
    """JSON íŒŒì¼ì—ì„œ ì„¤ì • ë¡œë“œ"""
    with open(filepath, 'r', encoding='utf-8') as f:
        config = json.load(f)
    logger.info(f"Config loaded from {filepath}")
    return config

def estimate_model_size(num_parameters: int, dtype: torch.dtype = torch.float16) -> str:
    """ëª¨ë¸ í¬ê¸° ì¶”ì •"""
    if dtype == torch.float16:
        bytes_per_param = 2
    elif dtype == torch.float32:
        bytes_per_param = 4
    elif dtype == torch.int8:
        bytes_per_param = 1
    else:
        bytes_per_param = 4  # ê¸°ë³¸ê°’
    
    total_bytes = num_parameters * bytes_per_param
    
    if total_bytes < 1024**3:
        return f"{total_bytes / 1024**2:.1f} MB"
    else:
        return f"{total_bytes / 1024**3:.1f} GB"

def format_time_duration(seconds: float) -> str:
    """ì‹œê°„ ì§€ì†ì‹œê°„ í¬ë§·íŒ…"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    
    if hours > 0:
        return f"{hours}ì‹œê°„ {minutes}ë¶„ {secs}ì´ˆ"
    elif minutes > 0:
        return f"{minutes}ë¶„ {secs}ì´ˆ"
    else:
        return f"{secs}ì´ˆ"

def get_system_info() -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘"""
    info = {
        "timestamp": datetime.now().isoformat(),
        "python_version": subprocess.check_output(["python3", "--version"]).decode().strip(),
        "torch_version": torch.__version__,
        "cuda_available": torch.cuda.is_available(),
        "system": {
            "cpu_count": psutil.cpu_count(),
            "memory_gb": psutil.virtual_memory().total / 1024**3,
            "disk_free_gb": psutil.disk_usage('.').free / 1024**3
        }
    }
    
    if torch.cuda.is_available():
        info["gpu"] = get_gpu_info()
    
    return info

def print_system_info():
    """ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥"""
    info = get_system_info()
    
    print("\n" + "="*50)
    print("ğŸ’» System Information")
    print("="*50)
    print(f"Python: {info['python_version']}")
    print(f"PyTorch: {info['torch_version']}")
    print(f"CUDA Available: {info['cuda_available']}")
    print(f"CPU Cores: {info['system']['cpu_count']}")
    print(f"RAM: {info['system']['memory_gb']:.1f} GB")
    print(f"Free Disk: {info['system']['disk_free_gb']:.1f} GB")
    
    if info['cuda_available']:
        print_gpu_info()
    
    print("="*50)

def validate_csv_file(csv_path: str) -> bool:
    """CSV íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬"""
    if not os.path.exists(csv_path):
        logger.error(f"CSV file not found: {csv_path}")
        return False
    
    try:
        import pandas as pd
        df = pd.read_csv(csv_path)
        
        required_columns = ['question', 'answer']
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        if missing_columns:
            logger.error(f"Missing required columns: {missing_columns}")
            return False
        
        if len(df) == 0:
            logger.error("CSV file is empty")
            return False
        
        logger.info(f"CSV file validation passed: {len(df)} rows")
        return True
        
    except Exception as e:
        logger.error(f"Error validating CSV file: {e}")
        return False

def backup_model(model_path: str, backup_dir: str = "./backups"):
    """ëª¨ë¸ ë°±ì—…"""
    if not os.path.exists(model_path):
        logger.warning(f"Model path not found: {model_path}")
        return
    
    create_directory(backup_dir)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_path = os.path.join(backup_dir, f"model_backup_{timestamp}")
    
    import shutil
    shutil.copytree(model_path, backup_path)
    logger.info(f"Model backed up to: {backup_path}")

def cleanup_checkpoints(output_dir: str, keep_latest: int = 2):
    """ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬"""
    if not os.path.exists(output_dir):
        return
    
    checkpoint_dirs = [
        d for d in os.listdir(output_dir)
        if d.startswith("checkpoint-") and os.path.isdir(os.path.join(output_dir, d))
    ]
    
    if len(checkpoint_dirs) <= keep_latest:
        return
    
    # ì²´í¬í¬ì¸íŠ¸ ë²ˆí˜¸ë¡œ ì •ë ¬
    checkpoint_dirs.sort(key=lambda x: int(x.split("-")[1]))
    
    # ì˜¤ë˜ëœ ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ
    for checkpoint_dir in checkpoint_dirs[:-keep_latest]:
        checkpoint_path = os.path.join(output_dir, checkpoint_dir)
        import shutil
        shutil.rmtree(checkpoint_path)
        logger.info(f"Removed old checkpoint: {checkpoint_path}")

class ProgressTracker:
    """ì§„í–‰ ìƒí™© ì¶”ì  í´ë˜ìŠ¤"""
    
    def __init__(self, total_steps: int):
        self.total_steps = total_steps
        self.current_step = 0
        self.start_time = datetime.now()
        
    def update(self, step: int):
        """ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸"""
        self.current_step = step
        
    def get_progress_info(self) -> Dict[str, Any]:
        """ì§„í–‰ ìƒí™© ì •ë³´ ë°˜í™˜"""
        elapsed_time = (datetime.now() - self.start_time).total_seconds()
        progress_ratio = self.current_step / self.total_steps if self.total_steps > 0 else 0
        
        if progress_ratio > 0:
            estimated_total_time = elapsed_time / progress_ratio
            remaining_time = estimated_total_time - elapsed_time
        else:
            remaining_time = 0
        
        return {
            "current_step": self.current_step,
            "total_steps": self.total_steps,
            "progress_percent": progress_ratio * 100,
            "elapsed_time": format_time_duration(elapsed_time),
            "remaining_time": format_time_duration(remaining_time) if remaining_time > 0 else "ê³„ì‚° ì¤‘...",
        }
    
    def print_progress(self):
        """ì§„í–‰ ìƒí™© ì¶œë ¥"""
        info = self.get_progress_info()
        print(f"ì§„í–‰ë¥ : {info['progress_percent']:.1f}% ({info['current_step']}/{info['total_steps']})")
        print(f"ê²½ê³¼ ì‹œê°„: {info['elapsed_time']}")
        print(f"ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {info['remaining_time']}")
