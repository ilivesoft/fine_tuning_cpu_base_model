#!/usr/bin/env python3
"""
ë©”ì¸ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸
Llama-3.2-Korean Fine-tuning for Civil Law Real Estate QA
"""

import os
import sys
import argparse
import logging
import json
from datetime import datetime

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸
from config import training_config, model_config
from utils import (
    setup_logging,
    check_system_requirements,
    print_system_info,
    set_environment_variables,
    validate_csv_file,
    create_directory,
    clear_gpu_memory,
    estimate_model_size,
    ProgressTracker
)
from model_manager import create_model_manager, check_gpu_memory
from data_loader import load_and_prepare_data
from trainer import create_training_manager, setup_training_environment, estimate_training_time

logger = logging.getLogger(__name__)


def parse_arguments():
    """ëª…ë ¹í–‰ ì¸ì íŒŒì‹±"""
    parser = argparse.ArgumentParser(description="Llama-3.2-Korean Fine-tuning")

    parser.add_argument("--csv_path", type=str, default=training_config.csv_path,
                        help="CSV ë°ì´í„° íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--output_dir", type=str, default=training_config.output_dir,
                        help="ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--model_name", type=str, default=model_config.model_name,
                        help="ë² ì´ìŠ¤ ëª¨ë¸ ì´ë¦„")
    parser.add_argument("--batch_size", type=int, default=training_config.batch_size,
                        help="ë°°ì¹˜ í¬ê¸°")
    parser.add_argument("--epochs", type=int, default=training_config.num_train_epochs,
                        help="í›ˆë ¨ ì—í­ ìˆ˜")
    parser.add_argument("--learning_rate", type=float, default=training_config.learning_rate,
                        help="í•™ìŠµë¥ ")
    parser.add_argument("--max_length", type=int, default=model_config.max_length,
                        help="ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´")
    parser.add_argument("--log_level", type=str, default="INFO",
                        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
                        help="ë¡œê·¸ ë ˆë²¨")
    parser.add_argument("--skip_validation", action="store_true",
                        help="ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ ê²€ì‚¬ ê±´ë„ˆë›°ê¸°")
    parser.add_argument("--dry_run", action="store_true",
                        help="ì‹¤ì œ í›ˆë ¨ ì—†ì´ ì„¤ì •ë§Œ í™•ì¸")

    return parser.parse_args()


def update_configs(args):
    """ì„¤ì • ì—…ë°ì´íŠ¸"""
    # í›ˆë ¨ ì„¤ì • ì—…ë°ì´íŠ¸
    training_config.csv_path = args.csv_path
    training_config.output_dir = args.output_dir
    training_config.batch_size = args.batch_size
    training_config.num_train_epochs = args.epochs
    training_config.learning_rate = args.learning_rate

    # ëª¨ë¸ ì„¤ì • ì—…ë°ì´íŠ¸
    model_config.model_name = args.model_name
    model_config.max_length = args.max_length

    logger.info("Configuration updated with command line arguments")
    logger.info(f"Using model: {model_config.model_name}")


def validate_setup(args):
    """ì„¤ì • ê²€ì¦"""
    logger.info("Validating setup...")

    # CSV íŒŒì¼ ê²€ì¦
    if not validate_csv_file(args.csv_path):
        logger.error("CSV file validation failed")
        return False

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    create_directory(args.output_dir)

    # ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸
    if not args.skip_validation:
        if not check_system_requirements():
            logger.error("System requirements check failed")
            return False

    logger.info("Setup validation completed")
    return True


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ì‹œì‘ ì‹œê°„ ê¸°ë¡
    start_time = datetime.now()

    # ëª…ë ¹í–‰ ì¸ì íŒŒì‹±
    args = parse_arguments()

    # ë¡œê¹… ì„¤ì •
    log_file = f"training_{start_time.strftime('%Y%m%d_%H%M%S')}.log"
    setup_logging(args.log_level, log_file)

    logger.info("=" * 60)
    logger.info("ğŸš€ Llama-3.2-Korean Fine-tuning Started")
    logger.info("=" * 60)

    try:
        # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
        set_environment_variables()

        # ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥
        print_system_info()

        # ì„¤ì • ì—…ë°ì´íŠ¸
        update_configs(args)

        # ì„¤ì • ê²€ì¦
        if not validate_setup(args):
            logger.error("Setup validation failed")
            sys.exit(1)

        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        clear_gpu_memory()

        # Dry run ëª¨ë“œ
        if args.dry_run:
            logger.info("ğŸ§ª Dry run mode - configuration validation only")
            logger.info(f"CSV file: {args.csv_path} âœ“")
            logger.info(f"Output directory: {args.output_dir} âœ“")
            logger.info(f"Model: {args.model_name} âœ“")
            logger.info(f"Batch size: {args.batch_size}")
            logger.info(f"Epochs: {args.epochs}")
            logger.info(f"Learning rate: {args.learning_rate}")
            logger.info("Configuration validation completed successfully!")
            return

        # í›ˆë ¨ í™˜ê²½ ì„¤ì •
        setup_training_environment()

        # 1. ëª¨ë¸ ë§¤ë‹ˆì € ìƒì„± ë° ëª¨ë¸ ë¡œë“œ
        logger.info("ğŸ“¦ Loading model and tokenizer...")
        model_manager = create_model_manager()
        model, tokenizer = model_manager.load_model_and_tokenizer()

        # ëª¨ë¸ í¬ê¸° ì •ë³´
        param_count = model.num_parameters()
        model_size = estimate_model_size(param_count, model_config.torch_dtype)
        logger.info(f"Model parameters: {param_count:,}")
        logger.info(f"Estimated model size: {model_size}")

        # 2. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
        logger.info("ğŸ“Š Loading and preprocessing data...")
        tokenized_dataset = load_and_prepare_data(tokenizer, args.csv_path)

        # í›ˆë ¨ ì‹œê°„ ì¶”ì •
        estimated_time = estimate_training_time(
            len(tokenized_dataset),
            args.batch_size * training_config.gradient_accumulation_steps,
            args.epochs
        )
        logger.info(f"â±ï¸ Estimated training time: {estimated_time}")

        # 3. íŠ¸ë ˆì´ë„ˆ ìƒì„±
        logger.info("ğŸ‹ï¸ Setting up trainer...")
        training_manager = create_training_manager(model, tokenizer)

        # 4. GPU ë©”ëª¨ë¦¬ í™•ì¸
        check_gpu_memory()

        # 5. í›ˆë ¨ ì‹œì‘
        logger.info("ğŸ¯ Starting training...")
        train_result = training_manager.train(tokenized_dataset, args.output_dir)

        # 6. ëª¨ë¸ ì €ì¥
        logger.info("ğŸ’¾ Saving final model...")
        training_manager.save_model(args.output_dir)

        # í›ˆë ¨ ì™„ë£Œ ì •ë³´
        end_time = datetime.now()
        total_time = end_time - start_time

        logger.info("=" * 60)
        logger.info("ğŸ‰ Training completed successfully!")
        logger.info(f"ğŸ“ Model saved to: {args.output_dir}")
        logger.info(f"â±ï¸ Total training time: {total_time}")
        logger.info(f"ğŸ“Š Final training loss: {train_result.training_loss:.4f}")
        logger.info(f"ğŸ“ˆ Total training steps: {train_result.global_step}")
        logger.info("=" * 60)

        # í›ˆë ¨ í†µê³„ ì €ì¥
        training_stats = {
            "start_time": start_time.isoformat(),
            "end_time": end_time.isoformat(),
            "total_time_seconds": total_time.total_seconds(),
            "final_loss": train_result.training_loss,
            "total_steps": train_result.global_step,
            "dataset_size": len(tokenized_dataset),
            "model_parameters": param_count,
            "config": {
                "batch_size": args.batch_size,
                "epochs": args.epochs,
                "learning_rate": args.learning_rate,
                "max_length": args.max_length,
                "gradient_accumulation_steps": training_config.gradient_accumulation_steps,
                "model_name": args.model_name
            }
        }

        stats_file = os.path.join(args.output_dir, "training_stats.json")
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(training_stats, f, indent=2, ensure_ascii=False, default=str)

        logger.info(f"ğŸ“ˆ Training statistics saved to: {stats_file}")

        # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
        logger.info("ğŸ§ª Running quick inference test...")
        from inference_manager import create_inference_manager

        inference_manager = create_inference_manager(model, tokenizer)
        test_question = "ì „ì„¸ ì‚¬ê¸°ë¥¼ ë‹¹í–ˆì„ ë•Œ ì–´ë–»ê²Œ ëŒ€ì‘í•´ì•¼ í•˜ë‚˜ìš”?"
        test_response = inference_manager.generate_response(test_question)

        print("\n" + "=" * 50)
        print("ğŸ§ª Quick Test Result")
        print("=" * 50)
        print(f"ì§ˆë¬¸: {test_question}")
        print(f"ë‹µë³€: {test_response}")
        print("=" * 50)

    except KeyboardInterrupt:
        logger.info("\nâŒ Training interrupted by user")
        sys.exit(1)

    except Exception as e:
        logger.error(f"âŒ Training failed with error: {e}")
        import traceback
        logger.error(traceback.format_exc())
        sys.exit(1)

    finally:
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        clear_gpu_memory()

        # ìµœì¢… ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
        if logger.isEnabledFor(logging.INFO):
            check_gpu_memory()


if __name__ == "__main__":
    main()