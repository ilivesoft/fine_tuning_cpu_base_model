#!/usr/bin/env python3
"""
ë©”ì¸ ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸
Fine-tuned Llama ëª¨ë¸ì„ ì‚¬ìš©í•œ ë²•ë¥  ìƒë‹´
"""

import os
import sys
import argparse
import logging
from datetime import datetime

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸
from config import model_config, inference_config
from utils import (
    setup_logging,
    print_system_info,
    set_environment_variables,
    clear_gpu_memory,
    create_directory
)
from model_manager import create_inference_manager, check_gpu_memory
from inference_manager import create_inference_manager as create_inf_mgr, create_chatbot, run_quick_test

logger = logging.getLogger(__name__)

def parse_arguments():
    """ëª…ë ¹í–‰ ì¸ì íŒŒì‹±"""
    parser = argparse.ArgumentParser(description="Fine-tuned Llama ëª¨ë¸ ì¶”ë¡ ")
    
    parser.add_argument("--model_path", type=str, required=True,
                       help="Fine-tuned ëª¨ë¸ ê²½ë¡œ")
    parser.add_argument("--base_model", type=str, default=model_config.model_name,
                       help="ë² ì´ìŠ¤ ëª¨ë¸ ì´ë¦„")
    parser.add_argument("--question", type=str,
                       help="ë‹¨ì¼ ì§ˆë¬¸")
    parser.add_argument("--questions_file", type=str,
                       help="ì§ˆë¬¸ë“¤ì´ ë‹´ê¸´ í…ìŠ¤íŠ¸ íŒŒì¼")
    parser.add_argument("--interactive", action="store_true",
                       help="ëŒ€í™”í˜• ëª¨ë“œ")
    parser.add_argument("--benchmark", action="store_true",
                       help="ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
    parser.add_argument("--quick_test", action="store_true",
                       help="ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
    parser.add_argument("--max_new_tokens", type=int, default=inference_config.max_new_tokens,
                       help="ìµœëŒ€ ìƒì„± í† í° ìˆ˜")
    parser.add_argument("--temperature", type=float, default=inference_config.temperature,
                       help="ìƒì„± ì˜¨ë„")
    parser.add_argument("--output_file", type=str,
                       help="ê²°ê³¼ ì €ì¥ íŒŒì¼")
    parser.add_argument("--log_level", type=str, default="INFO",
                       choices=["DEBUG", "INFO", "WARNING", "ERROR"],
                       help="ë¡œê·¸ ë ˆë²¨")
    
    return parser.parse_args()

def validate_arguments(args):
    """ì¸ì ìœ íš¨ì„± ê²€ì‚¬"""
    # ëª¨ë¸ ê²½ë¡œ í™•ì¸
    if not os.path.exists(args.model_path):
        logger.error(f"Model path does not exist: {args.model_path}")
        return False
    
    # ì§ˆë¬¸ íŒŒì¼ í™•ì¸
    if args.questions_file and not os.path.exists(args.questions_file):
        logger.error(f"Questions file does not exist: {args.questions_file}")
        return False
    
    # ëª¨ë“œ í™•ì¸
    modes = [args.question, args.questions_file, args.interactive, args.benchmark, args.quick_test]
    if sum(bool(mode) for mode in modes) == 0:
        logger.warning("No operation mode specified. Running quick test.")
        args.quick_test = True
    
    return True

def load_questions_from_file(filepath: str) -> list:
    """íŒŒì¼ì—ì„œ ì§ˆë¬¸ ëª©ë¡ ë¡œë“œ"""
    questions = []
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):  # ë¹ˆ ì¤„ê³¼ ì£¼ì„ ì œì™¸
                    questions.append(line)
        logger.info(f"Loaded {len(questions)} questions from {filepath}")
    except Exception as e:
        logger.error(f"Error loading questions from file: {e}")
    return questions

def save_results_to_file(results: list, filepath: str):
    """ê²°ê³¼ë¥¼ íŒŒì¼ì— ì €ì¥"""
    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write("# ë²•ë¥  ìƒë‹´ ê²°ê³¼\n")
            f.write(f"# ìƒì„± ì‹œê°„: {datetime.now().isoformat()}\n\n")
            
            for i, result in enumerate(results, 1):
                f.write(f"## ì§ˆë¬¸ {i}\n")
                f.write(f"**ì§ˆë¬¸:** {result['question']}\n\n")
                f.write(f"**ë‹µë³€:** {result['response']}\n\n")
                f.write("-" * 80 + "\n\n")
        
        logger.info(f"Results saved to {filepath}")
    except Exception as e:
        logger.error(f"Error saving results: {e}")

def run_single_question(inference_manager, question: str, args):
    """ë‹¨ì¼ ì§ˆë¬¸ ì²˜ë¦¬"""
    logger.info(f"Processing single question: {question[:50]}...")
    
    response = inference_manager.generate_response(
        question,
        max_new_tokens=args.max_new_tokens,
        temperature=args.temperature
    )
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*60)
    print("ğŸ“‹ Single Question Result")
    print("="*60)
    print(f"ì§ˆë¬¸: {question}")
    print(f"\në‹µë³€: {response}")
    print("="*60)
    
    return [{"question": question, "response": response}]

def run_multiple_questions(inference_manager, questions: list, args):
    """ì—¬ëŸ¬ ì§ˆë¬¸ ì²˜ë¦¬"""
    logger.info(f"Processing {len(questions)} questions...")
    
    results = []
    for i, question in enumerate(questions, 1):
        print(f"\nì§„í–‰ë¥ : {i}/{len(questions)}")
        print(f"ì§ˆë¬¸: {question}")
        
        response = inference_manager.generate_response(
            question,
            max_new_tokens=args.max_new_tokens,
            temperature=args.temperature
        )
        
        print(f"ë‹µë³€: {response}")
        print("-" * 60)
        
        results.append({
            "question": question,
            "response": response
        })
    
    return results

def run_benchmark_test(inference_manager):
    """ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    logger.info("Running benchmark test...")
    
    test_questions = [
        "ì „ì„¸ ì‚¬ê¸°ë¥¼ ë‹¹í–ˆì„ ë•Œ ì–´ë–»ê²Œ ëŒ€ì‘í•´ì•¼ í•˜ë‚˜ìš”?",
        "ì „ì„¸ê¶Œ ë“±ê¸°ì˜ íš¨ë ¥ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "ê¹¡í†µì „ì„¸ì˜ ìœ„í—˜ì„±ê³¼ ì˜ˆë°© ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”.",
        "ì „ì„¸ë³´ì¦ê¸ˆ ë°˜í™˜ë³´ì¦ë³´í—˜ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?",
        "ì„ëŒ€ì¸ì´ íŒŒì‚°í•œ ê²½ìš° ì „ì„¸ë³´ì¦ê¸ˆì„ ì–´ë–»ê²Œ íšŒìˆ˜í•˜ë‚˜ìš”?"
    ]
    
    benchmark_results = inference_manager.benchmark_generation(test_questions)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*60)
    print("ğŸ“Š Benchmark Results")
    print("="*60)
    print(f"í‰ê·  ìƒì„± ì‹œê°„: {benchmark_results['average_generation_time']:.2f}ì´ˆ")
    print(f"í‰ê·  í† í° ìˆ˜: {benchmark_results['average_token_count']:.1f}")
    print(f"ì´ˆë‹¹ í† í° ìˆ˜: {benchmark_results['tokens_per_second']:.1f}")
    print("\nì„¸ë¶€ ê²°ê³¼:")
    
    for i, (question, response, time, tokens) in enumerate(zip(
        benchmark_results['questions'],
        benchmark_results['responses'],
        benchmark_results['generation_times'],
        benchmark_results['token_counts']
    ), 1):
        print(f"\n{i}. ì§ˆë¬¸: {question}")
        print(f"   ë‹µë³€: {response}")
        print(f"   ì‹œê°„: {time:.2f}ì´ˆ, í† í°: {tokens}ê°œ")
    
    print("="*60)
    
    return benchmark_results

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ì‹œì‘ ì‹œê°„ ê¸°ë¡
    start_time = datetime.now()
    
    # ëª…ë ¹í–‰ ì¸ì íŒŒì‹±
    args = parse_arguments()
    
    # ë¡œê¹… ì„¤ì •
    log_file = f"inference_{start_time.strftime('%Y%m%d_%H%M%S')}.log"
    setup_logging(args.log_level, log_file)
    
    logger.info("="*60)
    logger.info("ğŸ”® Llama Inference Started")
    logger.info("="*60)
    
    try:
        # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
        set_environment_variables()
        
        # ì¸ì ìœ íš¨ì„± ê²€ì‚¬
        if not validate_arguments(args):
            logger.error("Argument validation failed")
            sys.exit(1)
        
        # ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥
        print_system_info()
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        clear_gpu_memory()
        
        # ëª¨ë¸ ë¡œë” ìƒì„± ë° ëª¨ë¸ ë¡œë“œ
        logger.info(f"ğŸ“¦ Loading fine-tuned model from: {args.model_path}")
        model_loader = create_inference_manager(args.model_path, args.base_model)
        model, tokenizer = model_loader.load_finetuned_model()
        
        # ì¶”ë¡  ë§¤ë‹ˆì € ìƒì„±
        inference_manager = create_inf_mgr(model, tokenizer)
        
        # ìƒì„± ì„¤ì • ì—…ë°ì´íŠ¸
        inference_manager.update_generation_config(
            max_new_tokens=args.max_new_tokens,
            temperature=args.temperature
        )
        
        # GPU ë©”ëª¨ë¦¬ í™•ì¸
        check_gpu_memory()
        
        results = []
        
        # ì‹¤í–‰ ëª¨ë“œì— ë”°ë¥¸ ì²˜ë¦¬
        if args.interactive:
            # ëŒ€í™”í˜• ëª¨ë“œ
            logger.info("ğŸ—£ï¸ Starting interactive mode...")
            chatbot = create_chatbot(inference_manager)
            conversation_history = inference_manager.interactive_chat()
            
            # ëŒ€í™” ê¸°ë¡ ì €ì¥
            if args.output_file:
                chatbot.save_conversation(args.output_file)
        
        elif args.benchmark:
            # ë²¤ì¹˜ë§ˆí¬ ëª¨ë“œ
            benchmark_results = run_benchmark_test(inference_manager)
            results = [{"benchmark": benchmark_results}]
        
        elif args.quick_test:
            # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ
            logger.info("ğŸ§ª Running quick test...")
            run_quick_test(inference_manager)
        
        elif args.question:
            # ë‹¨ì¼ ì§ˆë¬¸ ëª¨ë“œ
            results = run_single_question(inference_manager, args.question, args)
        
        elif args.questions_file:
            # ë‹¤ì¤‘ ì§ˆë¬¸ ëª¨ë“œ
            questions = load_questions_from_file(args.questions_file)
            if questions:
                results = run_multiple_questions(inference_manager, questions, args)
        
        # ê²°ê³¼ ì €ì¥
        if results and args.output_file and not args.interactive:
            save_results_to_file(results, args.output_file)
        
        # ì™„ë£Œ ì •ë³´
        end_time = datetime.now()
        total_time = end_time - start_time
        
        logger.info("="*60)
        logger.info("ğŸ‰ Inference completed successfully!")
        logger.info(f"â±ï¸ Total time: {total_time}")
        if args.output_file:
            logger.info(f"ğŸ“ Results saved to: {args.output_file}")
        logger.info("="*60)
        
    except KeyboardInterrupt:
        logger.info("\nâŒ Inference interrupted by user")
        sys.exit(1)
        
    except Exception as e:
        logger.error(f"âŒ Inference failed with error: {e}")
        import traceback
        logger.error(traceback.format_exc())
        sys.exit(1)
    
    finally:
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        clear_gpu_memory()

if __name__ == "__main__":
    main()
