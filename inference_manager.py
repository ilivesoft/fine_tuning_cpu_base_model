"""
Inference and text generation utilities
"""

import torch
import logging
from typing import Dict, List, Optional
from transformers import PreTrainedModel, PreTrainedTokenizer

from config_cpu import inference_config, format_prompt

logger = logging.getLogger(__name__)

class InferenceManager:
    """ì¶”ë¡  ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, model: PreTrainedModel, tokenizer: PreTrainedTokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.device = next(model.parameters()).device
        
        # ì¶”ë¡  ì„¤ì •
        self.generation_config = {
            "max_new_tokens": inference_config.max_new_tokens,
            "temperature": inference_config.temperature,
            "do_sample": inference_config.do_sample,
            "top_p": inference_config.top_p,
            "repetition_penalty": inference_config.repetition_penalty,
            "pad_token_id": tokenizer.eos_token_id,
            "eos_token_id": tokenizer.eos_token_id,
        }
        
    def generate_response(self, question: str, **generation_kwargs) -> str:
        """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±"""
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = format_prompt(question)
        
        # í† í¬ë‚˜ì´ì§•
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        
        # ìƒì„± ì„¤ì • ì—…ë°ì´íŠ¸
        gen_config = self.generation_config.copy()
        gen_config.update(generation_kwargs)
        
        try:
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    **gen_config
                )
            
            # ìƒˆë¡œ ìƒì„±ëœ í† í°ë§Œ ë””ì½”ë”©
            new_tokens = outputs[0][inputs.input_ids.shape[-1]:]
            response = self.tokenizer.decode(new_tokens, skip_special_tokens=True)
            
            # í›„ì²˜ë¦¬
            response = self._post_process_response(response)
            
            return response
            
        except Exception as e:
            logger.error(f"Error during generation: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def _post_process_response(self, response: str) -> str:
        """ì‘ë‹µ í›„ì²˜ë¦¬"""
        # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
        response = response.strip()
        
        # íŠ¹ìˆ˜ í† í° ì œê±° (í˜¹ì‹œ ë‚¨ì•„ìˆëŠ” ê²½ìš°)
        special_tokens = ["<|eot_id|>", "<|start_header_id|>", "<|end_header_id|>"]
        for token in special_tokens:
            response = response.replace(token, "")
        
        return response.strip()
    
    def generate_batch_responses(self, questions: List[str], **generation_kwargs) -> List[str]:
        """ì—¬ëŸ¬ ì§ˆë¬¸ì— ëŒ€í•œ ë°°ì¹˜ ì‘ë‹µ ìƒì„±"""
        responses = []
        
        for question in questions:
            try:
                response = self.generate_response(question, **generation_kwargs)
                responses.append(response)
            except Exception as e:
                logger.error(f"Error generating response for question: {question[:50]}...")
                responses.append("ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        
        return responses
    
    def interactive_chat(self):
        """ëŒ€í™”í˜• ì±„íŒ… ì¸í„°í˜ì´ìŠ¤"""
        print("\n" + "="*60)
        print("ğŸ  ë¯¼ë²• ë¶€ë™ì‚° ë²•ë¥  ìƒë‹´ ì±—ë´‡")
        print("="*60)
        print("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”. ì¢…ë£Œí•˜ë ¤ë©´ 'quit', 'exit', 'ì¢…ë£Œ', 'ë‚˜ê°€ê¸°'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        print("-"*60)
        
        conversation_history = []
        
        while True:
            try:
                # ì‚¬ìš©ì ì…ë ¥
                question = input("\nğŸ’¬ ì§ˆë¬¸: ").strip()
                
                # ì¢…ë£Œ ëª…ë ¹ í™•ì¸
                if question.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'ë‚˜ê°€ê¸°']:
                    print("\nğŸ‘‹ ìƒë‹´ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!")
                    break
                
                if not question:
                    print("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    continue
                
                # ë‹µë³€ ìƒì„±
                print("ğŸ¤” ë‹µë³€ ìƒì„± ì¤‘...")
                response = self.generate_response(question)
                
                # ê²°ê³¼ ì¶œë ¥
                print(f"\nğŸ›ï¸ ë‹µë³€:\n{response}")
                print("\n" + "-"*60)
                
                # ëŒ€í™” ê¸°ë¡ ì €ì¥
                conversation_history.append({
                    "question": question,
                    "response": response
                })
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ìƒë‹´ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"\nâŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                logger.error(f"Interactive chat error: {e}")
        
        return conversation_history
    
    def benchmark_generation(self, test_questions: List[str] = None) -> Dict:
        """ìƒì„± ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        if test_questions is None:
            test_questions = [
                "ì „ì„¸ ì‚¬ê¸°ë¥¼ ë‹¹í–ˆì„ ë•Œ ì–´ë–»ê²Œ ëŒ€ì‘í•´ì•¼ í•˜ë‚˜ìš”?",
                "ì „ì„¸ê¶Œ ë“±ê¸°ì˜ íš¨ë ¥ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                "ê¹¡í†µì „ì„¸ì˜ ìœ„í—˜ì„±ê³¼ ì˜ˆë°© ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”."
            ]
        
        logger.info("Running generation benchmark...")
        
        results = {
            "questions": [],
            "responses": [],
            "generation_times": [],
            "token_counts": []
        }
        
        for question in test_questions:
            start_time = torch.cuda.Event(enable_timing=True)
            end_time = torch.cuda.Event(enable_timing=True)
            
            # ì‹œê°„ ì¸¡ì • ì‹œì‘
            start_time.record()
            
            # ì‘ë‹µ ìƒì„±
            response = self.generate_response(question)
            
            # ì‹œê°„ ì¸¡ì • ì¢…ë£Œ
            end_time.record()
            torch.cuda.synchronize()
            
            generation_time = start_time.elapsed_time(end_time) / 1000.0  # ì´ˆ ë‹¨ìœ„
            token_count = len(self.tokenizer.encode(response))
            
            results["questions"].append(question)
            results["responses"].append(response)
            results["generation_times"].append(generation_time)
            results["token_counts"].append(token_count)
            
            logger.info(f"Generated {token_count} tokens in {generation_time:.2f}s")
        
        # í‰ê·  í†µê³„
        avg_time = sum(results["generation_times"]) / len(results["generation_times"])
        avg_tokens = sum(results["token_counts"]) / len(results["token_counts"])
        tokens_per_second = avg_tokens / avg_time if avg_time > 0 else 0
        
        results["average_generation_time"] = avg_time
        results["average_token_count"] = avg_tokens
        results["tokens_per_second"] = tokens_per_second
        
        logger.info(f"Benchmark results:")
        logger.info(f"  Average generation time: {avg_time:.2f}s")
        logger.info(f"  Average tokens: {avg_tokens:.1f}")
        logger.info(f"  Tokens per second: {tokens_per_second:.1f}")
        
        return results
    
    def update_generation_config(self, **kwargs):
        """ìƒì„± ì„¤ì • ì—…ë°ì´íŠ¸"""
        self.generation_config.update(kwargs)
        logger.info(f"Updated generation config: {kwargs}")

class ChatBot:
    """ë²•ë¥  ìƒë‹´ ì±—ë´‡ í´ë˜ìŠ¤"""
    
    def __init__(self, inference_manager: InferenceManager):
        self.inference_manager = inference_manager
        self.conversation_history = []
        
    def chat(self, question: str) -> str:
        """ë‹¨ì¼ ì§ˆë¬¸ ì²˜ë¦¬"""
        response = self.inference_manager.generate_response(question)
        
        # ëŒ€í™” ê¸°ë¡ ì €ì¥
        self.conversation_history.append({
            "question": question,
            "response": response,
            "timestamp": torch.cuda.Event(enable_timing=True)
        })
        
        return response
    
    def get_conversation_history(self) -> List[Dict]:
        """ëŒ€í™” ê¸°ë¡ ë°˜í™˜"""
        return self.conversation_history
    
    def clear_history(self):
        """ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"""
        self.conversation_history = []
        logger.info("Conversation history cleared")
    
    def save_conversation(self, filepath: str):
        """ëŒ€í™” ê¸°ë¡ ì €ì¥"""
        import json
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ëŠ” ì €ì¥ì—ì„œ ì œì™¸
        history_to_save = [
            {"question": item["question"], "response": item["response"]}
            for item in self.conversation_history
        ]
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(history_to_save, f, ensure_ascii=False, indent=2)
        
        logger.info(f"Conversation saved to {filepath}")

def create_inference_manager(model: PreTrainedModel, tokenizer: PreTrainedTokenizer) -> InferenceManager:
    """InferenceManager ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    return InferenceManager(model, tokenizer)

def create_chatbot(inference_manager: InferenceManager) -> ChatBot:
    """ChatBot ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    return ChatBot(inference_manager)

def run_quick_test(inference_manager: InferenceManager):
    """ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    test_questions = [
        "ì „ì„¸ ê³„ì•½ ì‹œ ì£¼ì˜ì‚¬í•­ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "ì „ì„¸ê¶Œê³¼ ì„ì°¨ê¶Œì˜ ì°¨ì´ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”."
    ]
    
    print("\n" + "="*50)
    print("ğŸ§ª Quick Test")
    print("="*50)
    
    for i, question in enumerate(test_questions, 1):
        print(f"\n{i}. ì§ˆë¬¸: {question}")
        response = inference_manager.generate_response(question)
        print(f"   ë‹µë³€: {response}")
        print("-" * 50)