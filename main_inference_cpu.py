#!/usr/bin/env python3
"""
Windows CPU ì „ìš© ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸
"""

import os
import sys
import argparse
import logging
from datetime import datetime

# GPU ë¹„í™œì„±í™”
os.environ["CUDA_VISIBLE_DEVICES"] = ""

from config_cpu import model_config, inference_config
from utils import (
    setup_logging,
    print_system_info,
    create_directory
)
from model_manager_cpu import create_inference_manager, check_cpu_memory
from inference_manager import create_inference_manager as create_inf_mgr, create_chatbot

logger = logging.getLogger(__name__)

def parse_arguments():
    """ëª…ë ¹í–‰ ì¸ì íŒŒì‹±"""
    parser = argparse.ArgumentParser(description="CPU ì „ìš© ì¶”ë¡ ")
    
    parser.add_argument("--model_path", type=str, required=True,
                       help="Fine-tuned ëª¨ë¸ ê²½ë¡œ")
    parser.add_argument("--base_model", type=str, default=model_config.model_name,
                       help="ë² ì´ìŠ¤ ëª¨ë¸ ì´ë¦„")
    parser.add_argument("--question", type=str,
                       help="ë‹¨ì¼ ì§ˆë¬¸")
    parser.add_argument("--interactive", action="store_true",
                       help="ëŒ€í™”í˜• ëª¨ë“œ")
    parser.add_argument("--quick_test", action="store_true",
                       help="ë¹ ë¥¸ í…ŒìŠ¤íŠ¸")
    parser.add_argument("--max_new_tokens", type=int, default=inference_config.max_new_tokens,
                       help="ìµœëŒ€ ìƒì„± í† í° ìˆ˜")
    parser.add_argument("--temperature", type=float, default=inference_config.temperature,
                       help="ìƒì„± ì˜¨ë„")
    parser.add_argument("--output_file", type=str,
                       help="ê²°ê³¼ ì €ì¥ íŒŒì¼")
    parser.add_argument("--log_level", type=str, default="INFO",
                       choices=["DEBUG", "INFO", "WARNING", "ERROR"],
                       help="ë¡œê·¸ ë ˆë²¨")
    
    return parser.parse_args()

def validate_arguments(args):
    """ì¸ì ìœ íš¨ì„± ê²€ì‚¬"""
    if not os.path.exists(args.model_path):
        logger.error(f"Model path does not exist: {args.model_path}")
        return False
    
    modes = [args.question, args.interactive, args.quick_test]
    if sum(bool(mode) for mode in modes) == 0:
        logger.warning("No operation mode specified. Running quick test.")
        args.quick_test = True
    
    return True

def run_single_question(inference_manager, question: str, args):
    """ë‹¨ì¼ ì§ˆë¬¸ ì²˜ë¦¬"""
    logger.info(f"Processing question: {question[:50]}...")
    
    response = inference_manager.generate_response(
        question,
        max_new_tokens=args.max_new_tokens,
        temperature=args.temperature
    )
    
    print("\n" + "="*60)
    print("ğŸ“‹ Result")
    print("="*60)
    print(f"ì§ˆë¬¸: {question}")
    print(f"\në‹µë³€: {response}")
    print("="*60)
    
    return [{"question": question, "response": response}]

def run_quick_test(inference_manager):
    """ë¹ ë¥¸ í…ŒìŠ¤íŠ¸"""
    test_questions = [
        "ì „ì„¸ê¶Œì´ë€ ë¬´ì—‡ì¸ê°€ìš”?",
        "ì „ì„¸ ì‚¬ê¸°ë¥¼ ì˜ˆë°©í•˜ëŠ” ë°©ë²•ì€?"
    ]
    
    print("\n" + "="*60)
    print("ğŸ§ª Quick Test")
    print("="*60)
    
    for i, question in enumerate(test_questions, 1):
        print(f"\n{i}. ì§ˆë¬¸: {question}")
        response = inference_manager.generate_response(question)
        print(f"   ë‹µë³€: {response}")
        print("-" * 60)

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    start_time = datetime.now()
    
    args = parse_arguments()
    
    # ë¡œê¹… ì„¤ì •
    log_file = f"inference_cpu_{start_time.strftime('%Y%m%d_%H%M%S')}.log"
    setup_logging(args.log_level, log_file)
    
    logger.info("="*60)
    logger.info("ğŸ–¥ï¸ CPU Inference Started")
    logger.info("="*60)
    
    try:
        # ì¸ì ê²€ì¦
        if not validate_arguments(args):
            logger.error("Argument validation failed")
            sys.exit(1)
        
        # ì‹œìŠ¤í…œ ì •ë³´
        print_system_info()
        
        # CPU ë©”ëª¨ë¦¬ í™•ì¸
        check_cpu_memory()
        
        # ëª¨ë¸ ë¡œë“œ
        logger.info(f"ğŸ“¦ Loading model from: {args.model_path}")
        model_loader = create_inference_manager(args.model_path, args.base_model)
        model, tokenizer = model_loader.load_finetuned_model()
        
        # ì¶”ë¡  ë§¤ë‹ˆì € ìƒì„±
        inference_manager = create_inf_mgr(model, tokenizer)
        
        # ìƒì„± ì„¤ì • ì—…ë°ì´íŠ¸
        inference_manager.update_generation_config(
            max_new_tokens=args.max_new_tokens,
            temperature=args.temperature
        )
        
        results = []
        
        # ì‹¤í–‰ ëª¨ë“œ
        if args.interactive:
            logger.info("ğŸ—£ï¸ Starting interactive mode...")
            chatbot = create_chatbot(inference_manager)
            conversation_history = inference_manager.interactive_chat()
            
            if args.output_file:
                chatbot.save_conversation(args.output_file)
        
        elif args.quick_test:
            logger.info("ğŸ§ª Running quick test...")
            run_quick_test(inference_manager)
        
        elif args.question:
            results = run_single_question(inference_manager, args.question, args)
        
        # ì™„ë£Œ ì •ë³´
        end_time = datetime.now()
        total_time = end_time - start_time
        
        logger.info("="*60)
        logger.info("ğŸ‰ Inference completed!")
        logger.info(f"â±ï¸ Total time: {total_time}")
        if args.output_file:
            logger.info(f"ğŸ“ Results saved to: {args.output_file}")
        logger.info("="*60)
        
    except KeyboardInterrupt:
        logger.info("\nâŒ Interrupted by user")
        sys.exit(1)
        
    except Exception as e:
        logger.error(f"âŒ Inference failed: {e}")
        import traceback
        logger.error(traceback.format_exc())
        sys.exit(1)

if __name__ == "__main__":
    main()
