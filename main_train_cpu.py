#!/usr/bin/env python3
"""
Windows CPU ì „ìš© í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸
"""

import os
import sys
import argparse
import logging
import json
from datetime import datetime

# GPU ë¹„í™œì„±í™”
os.environ["CUDA_VISIBLE_DEVICES"] = ""

# CPU ì „ìš© ëª¨ë“ˆ ì„í¬íŠ¸
from config_cpu import training_config, model_config
from utils import (
    setup_logging,
    print_system_info,
    validate_csv_file,
    create_directory,
    ProgressTracker
)
from model_manager_cpu import create_cpu_model_manager, check_cpu_memory
from data_loader import load_and_prepare_data
from trainer import create_training_manager

logger = logging.getLogger(__name__)

def parse_arguments():
    """ëª…ë ¹í–‰ ì¸ì íŒŒì‹±"""
    parser = argparse.ArgumentParser(description="CPU ì „ìš© Llama Fine-tuning")
    
    parser.add_argument("--csv_path", type=str, default=training_config.csv_path,
                       help="CSV ë°ì´í„° íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--output_dir", type=str, default=training_config.output_dir,
                       help="ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--model_name", type=str, default=model_config.model_name,
                       help="ë² ì´ìŠ¤ ëª¨ë¸ ì´ë¦„")
    parser.add_argument("--batch_size", type=int, default=training_config.batch_size,
                       help="ë°°ì¹˜ í¬ê¸°")
    parser.add_argument("--epochs", type=int, default=training_config.num_train_epochs,
                       help="í›ˆë ¨ ì—í­ ìˆ˜")
    parser.add_argument("--max_steps", type=int, default=training_config.max_steps,
                       help="ìµœëŒ€ ìŠ¤í… ìˆ˜ (CPU ìµœì í™”)")
    parser.add_argument("--learning_rate", type=float, default=training_config.learning_rate,
                       help="í•™ìŠµë¥ ")
    parser.add_argument("--save_steps", type=int, default=training_config.save_steps,
                       help="ì €ì¥ ê°„ê²©")
    parser.add_argument("--log_level", type=str, default="INFO",
                       choices=["DEBUG", "INFO", "WARNING", "ERROR"],
                       help="ë¡œê·¸ ë ˆë²¨")
    parser.add_argument("--dry_run", action="store_true",
                       help="ì„¤ì • ê²€ì¦ë§Œ ì‹¤í–‰")
    
    return parser.parse_args()

def update_configs(args):
    """ì„¤ì • ì—…ë°ì´íŠ¸"""
    training_config.csv_path = args.csv_path
    training_config.output_dir = args.output_dir
    training_config.batch_size = args.batch_size
    training_config.num_train_epochs = args.epochs
    training_config.max_steps = args.max_steps
    training_config.learning_rate = args.learning_rate
    training_config.save_steps = args.save_steps
    
    model_config.model_name = args.model_name
    
    logger.info("Configuration updated")
    logger.info(f"Model: {model_config.model_name}")
    logger.info(f"Max steps: {training_config.max_steps}")

def validate_setup(args):
    """ì„¤ì • ê²€ì¦"""
    logger.info("Validating setup...")
    
    # CSV íŒŒì¼ ê²€ì¦
    if not validate_csv_file(args.csv_path):
        logger.error("CSV file validation failed")
        return False
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    create_directory(args.output_dir)
    
    logger.info("Setup validation completed")
    return True

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    start_time = datetime.now()
    
    # ëª…ë ¹í–‰ ì¸ì íŒŒì‹±
    args = parse_arguments()
    
    # ë¡œê¹… ì„¤ì •
    log_file = f"training_cpu_{start_time.strftime('%Y%m%d_%H%M%S')}.log"
    setup_logging(args.log_level, log_file)
    
    logger.info("=" * 60)
    logger.info("ğŸ–¥ï¸ CPU ì „ìš© Llama Fine-tuning Started")
    logger.info("=" * 60)
    
    try:
        # ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥
        print_system_info()
        
        # ì„¤ì • ì—…ë°ì´íŠ¸
        update_configs(args)
        
        # ì„¤ì • ê²€ì¦
        if not validate_setup(args):
            logger.error("Setup validation failed")
            sys.exit(1)
        
        # Dry run ëª¨ë“œ
        if args.dry_run:
            logger.info("ğŸ§ª Dry run mode - configuration validation only")
            logger.info(f"CSV file: {args.csv_path} âœ“")
            logger.info(f"Output directory: {args.output_dir} âœ“")
            logger.info(f"Model: {args.model_name} âœ“")
            logger.info(f"Max steps: {args.max_steps}")
            logger.info("Configuration validation completed!")
            return
        
        # CPU ë©”ëª¨ë¦¬ í™•ì¸
        check_cpu_memory()
        
        # 1. ëª¨ë¸ ë§¤ë‹ˆì € ìƒì„± ë° ëª¨ë¸ ë¡œë“œ
        logger.info("ğŸ“¦ Loading model and tokenizer on CPU...")
        model_manager = create_cpu_model_manager()
        model, tokenizer = model_manager.load_model_and_tokenizer()
        
        # 2. ë°ì´í„° ë¡œë“œ
        logger.info("ğŸ“Š Loading and preprocessing data...")
        tokenized_dataset = load_and_prepare_data(tokenizer, args.csv_path)
        
        # 3. íŠ¸ë ˆì´ë„ˆ ìƒì„±
        logger.info("ğŸ‹ï¸ Setting up trainer...")
        training_manager = create_training_manager(model, tokenizer)
        
        # 4. í›ˆë ¨ ì‹œì‘
        logger.info("ğŸ¯ Starting training on CPU...")
        logger.warning("âš ï¸ CPU training is slow. Please be patient...")
        
        train_result = training_manager.train(tokenized_dataset, args.output_dir)
        
        # 5. ëª¨ë¸ ì €ì¥
        logger.info("ğŸ’¾ Saving final model...")
        training_manager.save_model(args.output_dir)
        
        # í›ˆë ¨ ì™„ë£Œ ì •ë³´
        end_time = datetime.now()
        total_time = end_time - start_time
        
        logger.info("=" * 60)
        logger.info("ğŸ‰ Training completed successfully!")
        logger.info(f"ğŸ“ Model saved to: {args.output_dir}")
        logger.info(f"â±ï¸ Total time: {total_time}")
        logger.info(f"ğŸ“Š Final loss: {train_result.training_loss:.4f}")
        logger.info(f"ğŸ“ˆ Total steps: {train_result.global_step}")
        logger.info("=" * 60)
        
        # í›ˆë ¨ í†µê³„ ì €ì¥
        training_stats = {
            "start_time": start_time.isoformat(),
            "end_time": end_time.isoformat(),
            "total_time_seconds": total_time.total_seconds(),
            "final_loss": train_result.training_loss,
            "total_steps": train_result.global_step,
            "dataset_size": len(tokenized_dataset),
            "device": "cpu",
            "config": {
                "batch_size": args.batch_size,
                "max_steps": args.max_steps,
                "learning_rate": args.learning_rate,
                "model_name": args.model_name
            }
        }
        
        stats_file = os.path.join(args.output_dir, "training_stats.json")
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(training_stats, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"ğŸ“ˆ Training statistics saved to: {stats_file}")
        
    except KeyboardInterrupt:
        logger.info("\nâŒ Training interrupted by user")
        sys.exit(1)
        
    except Exception as e:
        logger.error(f"âŒ Training failed: {e}")
        import traceback
        logger.error(traceback.format_exc())
        sys.exit(1)

if __name__ == "__main__":
    main()
