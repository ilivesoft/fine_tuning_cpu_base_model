# Windows 10 CPU ì „ìš© Llama íŒŒì¸íŠœë‹ ì„¤ì • ê°€ì´ë“œ

## ğŸ–¥ï¸ CPU ì „ìš© ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

### ìµœì†Œ ìš”êµ¬ì‚¬í•­
- **OS**: Windows 10 (64-bit)
- **CPU**: Intel i5-8ì„¸ëŒ€ ì´ìƒ ë˜ëŠ” AMD Ryzen 5 3ì„¸ëŒ€ ì´ìƒ
- **RAM**: 32GB ì´ìƒ **í•„ìˆ˜** (GPU VRAM ëŒ€ì‹  ì‚¬ìš©)
- **ì €ì¥ê³µê°„**: ìµœì†Œ 100GB ì—¬ìœ  ê³µê°„ (SSD ê¶Œì¥)

### ê¶Œì¥ ìš”êµ¬ì‚¬í•­
- **CPU**: Intel i7/i9 ë˜ëŠ” AMD Ryzen 7/9 (ë©€í‹°ì½”ì–´ ê³ ì„±ëŠ¥)
- **RAM**: 64GB ì´ìƒ
- **ì €ì¥ê³µê°„**: NVMe SSD 200GB ì´ìƒ

> âš ï¸ **ì¤‘ìš”**: CPU í›ˆë ¨ì€ GPUë³´ë‹¤ 20-50ë°° ëŠë¦½ë‹ˆë‹¤. ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ í›„ ì§„í–‰í•˜ì„¸ìš”.

---

## ğŸ”§ 1ë‹¨ê³„: ê¸°ë³¸ ì†Œí”„íŠ¸ì›¨ì–´ ì„¤ì¹˜

### 1.1 Python ì„¤ì¹˜
```cmd
# Python 3.11 ë‹¤ìš´ë¡œë“œ ë° ì„¤ì¹˜
# https://www.python.org/downloads/windows/
# "Add Python to PATH" ë°˜ë“œì‹œ ì²´í¬

# ì„¤ì¹˜ í™•ì¸
python --version
pip --version
```

### 1.2 Git ì„¤ì¹˜ (ì„ íƒì‚¬í•­)
- [Git for Windows](https://git-scm.com/download/win)

---

## ğŸ“¦ 2ë‹¨ê³„: CPU ì „ìš© í™˜ê²½ ì„¤ì •

### 2.1 ê°€ìƒí™˜ê²½ ìƒì„±
```cmd
# í”„ë¡œì íŠ¸ í´ë” ìƒì„±
mkdir D:\Projects\llama_cpu_project
cd /d D:\Projects\llama_cpu_project

# ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv llama_cpu_env

# ê°€ìƒí™˜ê²½ í™œì„±í™”
llama_cpu_env\Scripts\activate
```

### 2.2 CPU ë²„ì „ PyTorch ì„¤ì¹˜
```cmd
# CPU ì „ìš© PyTorch ì„¤ì¹˜ (CUDA ì—†ìŒ)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu

# ì„¤ì¹˜ í™•ì¸
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CPU only: {not torch.cuda.is_available()}')"
```

### 2.3 í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
```cmd
# í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ (CPU ìµœì í™”)
pip install transformers>=4.53.0
pip install accelerate>=1.8.1
pip install peft>=0.16.0
pip install datasets>=3.6.0

# ë°ì´í„° ì²˜ë¦¬
pip install pandas numpy pyarrow

# ìœ í‹¸ë¦¬í‹°
pip install tqdm psutil
pip install tokenizers safetensors huggingface-hub
pip install Jinja2 PyYAML requests
```

> ğŸ“ **ì£¼ì˜**: `bitsandbytes`ëŠ” ì„¤ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ (GPU ì „ìš©)

---

## âš™ï¸ 3ë‹¨ê³„: CPU ì „ìš© ì„¤ì • íŒŒì¼ ìˆ˜ì •

### 3.1 `config_cpu.py` ìƒì„±
ê¸°ì¡´ `config.py` ëŒ€ì‹  CPU ìµœì í™” ë²„ì „:

```python
"""
CPU ì „ìš© ì„¤ì • íŒŒì¼
"""

import torch
from dataclasses import dataclass
from typing import List

@dataclass
class ModelConfig:
    """CPU ìµœì í™” ëª¨ë¸ ì„¤ì •"""
    # ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš© ê¶Œì¥
    model_name: str = "microsoft/DialoGPT-small"  # ì‘ì€ ëª¨ë¸ë¡œ ë³€ê²½
    max_length: int = 256  # ì‹œí€€ìŠ¤ ê¸¸ì´ ë‹¨ì¶•
    torch_dtype: torch.dtype = torch.float32  # CPUëŠ” float32 ê¶Œì¥
    use_8bit: bool = False  # CPUì—ì„œëŠ” ë¶ˆê°€ëŠ¥
    trust_remote_code: bool = True
    low_cpu_mem_usage: bool = True

@dataclass
class LoRAConfig:
    """CPU ìµœì í™” LoRA ì„¤ì •"""
    r: int = 8  # rank ê°ì†Œë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
    lora_alpha: int = 16  # ë¹„ë¡€ì ìœ¼ë¡œ ê°ì†Œ
    lora_dropout: float = 0.1
    target_modules: List[str] = None

    def __post_init__(self):
        if self.target_modules is None:
            # ë” ì ì€ ëª¨ë“ˆ íƒ€ê²ŸíŒ…
            self.target_modules = [
                "c_attn", "c_proj"  # DialoGPTìš©
            ]

@dataclass
class TrainingConfig:
    """CPU ìµœì í™” í›ˆë ¨ ì„¤ì •"""
    csv_path: str = "civil_law_qa_dataset.csv"
    output_dir: str = "./fine_tuned_model_cpu"
    
    # CPU ìµœì í™” ë°°ì¹˜ ì„¤ì •
    batch_size: int = 1  # ìµœì†Œ ë°°ì¹˜ í¬ê¸°
    gradient_accumulation_steps: int = 32  # íš¨ê³¼ì  ë°°ì¹˜ í¬ê¸° = 32
    
    # í•™ìŠµë¥  ë° ìŠ¤ì¼€ì¤„ëŸ¬
    learning_rate: float = 5e-5  # ë” ë‚®ì€ í•™ìŠµë¥ 
    lr_scheduler_type: str = "linear"
    warmup_ratio: float = 0.1
    
    # ì—í­ ì„¤ì • (CPUëŠ” ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼)
    num_train_epochs: int = 1  # ì¼ë‹¨ 1 ì—í­ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
    max_steps: int = 100  # ë˜ëŠ” ì œí•œëœ ìŠ¤í… ìˆ˜
    
    # ì €ì¥ ì„¤ì •
    save_strategy: str = "steps"
    save_steps: int = 50
    save_total_limit: int = 2
    
    # ë¡œê¹…
    logging_steps: int = 10
    logging_strategy: str = "steps"
    
    # CPU ìµœì í™”
    fp16: bool = False  # CPUëŠ” FP16 ë¯¸ì§€ì›
    gradient_checkpointing: bool = False
    dataloader_pin_memory: bool = False
    dataloader_num_workers: int = 0  # CPUë§Œ ì‚¬ìš©
    remove_unused_columns: bool = False
    
    # ê¸°íƒ€
    seed: int = 42
    report_to: str = None

@dataclass
class InferenceConfig:
    """CPU ìµœì í™” ì¶”ë¡  ì„¤ì •"""
    max_new_tokens: int = 256  # ë” ì ì€ í† í°
    temperature: float = 0.8
    do_sample: bool = True
    top_p: float = 0.9
    repetition_penalty: float = 1.1

@dataclass
class SystemConfig:
    """CPU ì „ìš© ì‹œìŠ¤í…œ ì„¤ì •"""
    device: str = "cpu"  # ê°•ì œë¡œ CPU ì‚¬ìš©
    cuda_visible_devices: str = ""  # GPU ë¹„í™œì„±í™”
    pytorch_cuda_alloc_conf: str = ""
    tokenizers_parallelism: bool = True  # CPUëŠ” ë³‘ë ¬í™” ê°€ëŠ¥

# ì „ì—­ ì„¤ì • ì¸ìŠ¤í„´ìŠ¤
model_config = ModelConfig()
lora_config = LoRAConfig()
training_config = TrainingConfig()
inference_config = InferenceConfig()
system_config = SystemConfig()

def get_system_prompt() -> str:
    """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë°˜í™˜"""
    return "ë‹¹ì‹ ì€ í•œêµ­ ë¯¼ë²•, íŠ¹íˆ ë¶€ë™ì‚°ê³¼ ì „ì„¸ ê´€ë ¨ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë²•ë¥  ì¡°ì–¸ì„ ì œê³µí•´ì£¼ì„¸ìš”."

def format_prompt(question: str, answer: str = None) -> str:
    """ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ… (CPU ìµœì í™”)"""
    if answer:
        # í›ˆë ¨ìš©
        return f"ì§ˆë¬¸: {question}\në‹µë³€: {answer}"
    else:
        # ì¶”ë¡ ìš©
        return f"ì§ˆë¬¸: {question}\në‹µë³€:"
```

### 3.2 `model_manager_cpu.py` ìƒì„±
CPU ì „ìš© ëª¨ë¸ ë§¤ë‹ˆì €:

```python
"""
CPU ì „ìš© ëª¨ë¸ ê´€ë¦¬
"""

import torch
import logging
from transformers import AutoTokenizer, AutoModelForCausalLM, GPT2LMHeadModel
from peft import LoraConfig, get_peft_model, TaskType, PeftModel
from typing import Tuple

from config_cpu import model_config, lora_config

logger = logging.getLogger(__name__)

class CPUModelManager:
    """CPU ì „ìš© ëª¨ë¸ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        # ê°•ì œë¡œ CPU ì‚¬ìš©
        self.device = torch.device("cpu")
        self.tokenizer = None
        self.model = None
        
    def load_tokenizer(self, model_name: str = None) -> AutoTokenizer:
        """í† í¬ë‚˜ì´ì € ë¡œë“œ"""
        if model_name is None:
            model_name = model_config.model_name
            
        logger.info(f"Loading tokenizer: {model_name}")
        
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=model_config.trust_remote_code,
            padding_side="right"
        )
        
        # íŒ¨ë”© í† í° ì„¤ì •
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            
        self.tokenizer = tokenizer
        logger.info("Tokenizer loaded successfully")
        return tokenizer
    
    def load_base_model(self, model_name: str = None) -> AutoModelForCausalLM:
        """CPU ì „ìš© ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ"""
        if model_name is None:
            model_name = model_config.model_name
            
        logger.info(f"Loading base model on CPU: {model_name}")
        
        # CPU ì „ìš© ë¡œë”©
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=model_config.torch_dtype,  # float32
            device_map={"": self.device},  # ëª…ì‹œì ìœ¼ë¡œ CPU ì§€ì •
            trust_remote_code=model_config.trust_remote_code,
            low_cpu_mem_usage=model_config.low_cpu_mem_usage,
        )
        
        # CPUë¡œ ì´ë™
        model = model.to(self.device)
        
        self.model = model
        
        # ëª¨ë¸ ì •ë³´ ì¶œë ¥
        total_params = model.num_parameters()
        logger.info(f"Model loaded on {self.device}")
        logger.info(f"Total parameters: {total_params:,}")
        logger.info(f"Model dtype: {model.dtype}")
        
        return model
    
    def setup_lora(self, model: AutoModelForCausalLM = None) -> AutoModelForCausalLM:
        """CPU ìµœì í™” LoRA ì„¤ì •"""
        if model is None:
            model = self.model
            
        logger.info("Setting up LoRA for CPU...")
        
        # CPU ìµœì í™” LoRA ì„¤ì •
        peft_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=lora_config.r,  # 8ë¡œ ê°ì†Œ
            lora_alpha=lora_config.lora_alpha,  # 16ìœ¼ë¡œ ê°ì†Œ
            lora_dropout=lora_config.lora_dropout,
            target_modules=lora_config.target_modules,
            bias="none"
        )
        
        # LoRA ëª¨ë¸ ìƒì„±
        model = get_peft_model(model, peft_config)
        model = model.to(self.device)  # CPUë¡œ í™•ì‹¤íˆ ì´ë™
        
        # í›ˆë ¨ ëª¨ë“œë¡œ ì„¤ì •
        model.train()
        
        # í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ì •ë³´ ì¶œë ¥
        model.print_trainable_parameters()
        
        self.model = model
        logger.info("LoRA setup completed for CPU")
        
        return model
    
    def load_model_and_tokenizer(self) -> Tuple[AutoModelForCausalLM, AutoTokenizer]:
        """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ í•¨ê»˜ ë¡œë“œ"""
        logger.info("Loading model and tokenizer for CPU...")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = self.load_tokenizer()
        
        # ëª¨ë¸ ë¡œë“œ
        model = self.load_base_model()
        
        # LoRA ì„¤ì •
        model = self.setup_lora(model)
        
        logger.info("Model and tokenizer loaded successfully on CPU")
        return model, tokenizer

def create_cpu_model_manager() -> CPUModelManager:
    """CPUModelManager ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    return CPUModelManager()

def check_cpu_info():
    """CPU ì •ë³´ í™•ì¸"""
    import psutil
    
    cpu_count = psutil.cpu_count(logical=False)  # ë¬¼ë¦¬ì  ì½”ì–´
    cpu_count_logical = psutil.cpu_count(logical=True)  # ë…¼ë¦¬ì  ì½”ì–´
    memory_gb = psutil.virtual_memory().total / 1024**3
    
    logger.info(f"Physical CPU cores: {cpu_count}")
    logger.info(f"Logical CPU cores: {cpu_count_logical}")
    logger.info(f"Total RAM: {memory_gb:.1f} GB")
    logger.info(f"Available RAM: {psutil.virtual_memory().available / 1024**3:.1f} GB")
    
    # PyTorch ë©€í‹°ìŠ¤ë ˆë”© ì„¤ì •
    torch.set_num_threads(cpu_count)  # ë¬¼ë¦¬ì  ì½”ì–´ ìˆ˜ë¡œ ì„¤ì •
    logger.info(f"PyTorch threads set to: {torch.get_num_threads()}")
```

---

## ğŸš€ 4ë‹¨ê³„: CPU ì „ìš© ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

### 4.1 `run_cpu_scripts.bat` ìƒì„±

```batch
@echo off
setlocal enabledelayedexpansion

REM CPU ì „ìš© í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
set CUDA_VISIBLE_DEVICES=
set OMP_NUM_THREADS=8
set MKL_NUM_THREADS=8
set TOKENIZERS_PARALLELISM=true

echo [94m================================[0m
echo [94m   CPU ì „ìš© Llama Fine-tuning  [0m
echo [94m================================[0m

if "%1"=="" goto :show_help
if "%1"=="help" goto :show_help
if "%1"=="check" goto :check_system
if "%1"=="train_small" goto :train_small
if "%1"=="train_test" goto :train_test
if "%1"=="chat" goto :chat
if "%1"=="test" goto :test

:show_help
echo.
echo [93mì‚¬ìš©ë²•:[0m
echo   %0 [ëª…ë ¹]
echo.
echo [93mëª…ë ¹ì–´:[0m
echo   check       - ì‹œìŠ¤í…œ ì •ë³´ í™•ì¸
echo   train_test  - í…ŒìŠ¤íŠ¸ í›ˆë ¨ (10 ìŠ¤í…)
echo   train_small - ì†Œê·œëª¨ í›ˆë ¨ (100 ìŠ¤í…)
echo   chat        - ëŒ€í™”í˜• ì¶”ë¡ 
echo   test        - ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
echo   help        - ë„ì›€ë§
echo.
echo [91mì£¼ì˜: CPU í›ˆë ¨ì€ ë§¤ìš° ëŠë¦½ë‹ˆë‹¤![0m
goto :end

:check_system
echo [96mì‹œìŠ¤í…œ ì •ë³´ í™•ì¸ ì¤‘...[0m
python -c "import torch, psutil; print(f'CPU cores: {psutil.cpu_count()}'); print(f'RAM: {psutil.virtual_memory().total/1024**3:.1f}GB'); print(f'PyTorch CPU only: {not torch.cuda.is_available()}')"
goto :end

:train_test
echo [92mí…ŒìŠ¤íŠ¸ í›ˆë ¨ ì‹œì‘ (10 ìŠ¤í…)...[0m
python main_train_cpu.py --max_steps 10 --save_steps 5
goto :end

:train_small
echo [92mì†Œê·œëª¨ í›ˆë ¨ ì‹œì‘ (100 ìŠ¤í…)...[0m
python main_train_cpu.py --max_steps 100 --save_steps 25
goto :end

:chat
echo [92mëŒ€í™”í˜• ì¶”ë¡  ì‹œì‘...[0m
python main_inference_cpu.py --model_path ./fine_tuned_model_cpu --interactive
goto :end

:test
echo [92më¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì‹œì‘...[0m
python main_inference_cpu.py --model_path ./fine_tuned_model_cpu --quick_test
goto :end

:end
pause
```

---

## ğŸ“ 5ë‹¨ê³„: CPU ìµœì í™”ëœ ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸ ìˆ˜ì •

### 5.1 ì£¼ìš” ìˆ˜ì •ì‚¬í•­

#### `main_train_cpu.py` (main_train.py ìˆ˜ì • ë²„ì „)
```python
# ë§¨ ìœ„ì— CPU ì„¤ì • ì¶”ê°€
import os
os.environ["CUDA_VISIBLE_DEVICES"] = ""  # GPU ë¹„í™œì„±í™”

# config.py ëŒ€ì‹  config_cpu.py import
from config_cpu import training_config, model_config
from model_manager_cpu import create_cpu_model_manager, check_cpu_info
```

#### ì¶”ê°€ ìµœì í™” ì„¤ì •
```python
# CPU ë©€í‹°ìŠ¤ë ˆë”© ìµœì í™”
import torch
torch.set_num_threads(8)  # CPU ì½”ì–´ ìˆ˜ì— ë§ê²Œ ì¡°ì •

# ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•œ ì„¤ì •
torch.backends.cudnn.enabled = False  # CUDNN ë¹„í™œì„±í™”
```

---

## âš ï¸ 6ë‹¨ê³„: CPU í›ˆë ¨ ì£¼ì˜ì‚¬í•­ ë° ìµœì í™”

### 6.1 í˜„ì‹¤ì ì¸ ê¸°ëŒ€ì¹˜ ì„¤ì •

```cmd
# ğŸŒ ì†ë„ ë¹„êµ (ì°¸ê³ ìš©)
# GPU (RTX 4060): 1 epoch â‰ˆ 30ë¶„
# CPU (i7-10ì„¸ëŒ€): 1 epoch â‰ˆ 10-20ì‹œê°„

# ë”°ë¼ì„œ ì²˜ìŒì—ëŠ” ë§¤ìš° ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
```

### 6.2 ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •

#### Windows ê°€ìƒ ë©”ëª¨ë¦¬ ì„¤ì •
1. **ì œì–´íŒ** â†’ **ì‹œìŠ¤í…œ** â†’ **ê³ ê¸‰ ì‹œìŠ¤í…œ ì„¤ì •**
2. **ì„±ëŠ¥** â†’ **ì„¤ì •** â†’ **ê³ ê¸‰** â†’ **ê°€ìƒ ë©”ëª¨ë¦¬**
3. **ì‚¬ìš©ì ì§€ì • í¬ê¸°**: ì´ˆê¸°ê°’ 16GB, ìµœëŒ€ê°’ 32GB ì„¤ì •

#### ì‹œìŠ¤í…œ ìµœì í™”
```cmd
# ë°±ê·¸ë¼ìš´ë“œ ì•± ì¢…ë£Œ
taskkill /f /im chrome.exe
taskkill /f /im firefox.exe

# ê³ ì„±ëŠ¥ ì „ì› ëª¨ë“œ ì„¤ì •
powercfg /setactive 8c5e7fda-e8bf-4a96-9a85-a6e23a8c635c
```

### 6.3 ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ í…ŒìŠ¤íŠ¸

#### `test_dataset.csv` ìƒì„± (10ê°œ ìƒ˜í”Œë§Œ)
```csv
question,answer,category,difficulty
"ì „ì„¸ê¶Œì´ë€ ë¬´ì—‡ì¸ê°€ìš”?","ì „ì„¸ê¶Œì€ ì „ì„¸ê¸ˆì„ ì§€ê¸‰í•˜ê³  íƒ€ì¸ì˜ ë¶€ë™ì‚°ì„ ì ìœ í•˜ì—¬ ì‚¬ìš©Â·ìˆ˜ìµí•  ìˆ˜ ìˆëŠ” ë¬¼ê¶Œì…ë‹ˆë‹¤.","ë¶€ë™ì‚°ë¬¼ê¶Œ","ì´ˆê¸‰"
"ì „ì„¸ ì‚¬ê¸° ì˜ˆë°©ë²•ì€?","ì „ì„¸ê¶Œ ë“±ê¸°, í™•ì •ì¼ì, ì „ì„¸ë³´ì¦ë³´í—˜ ê°€ì… ë“±ì´ ìˆìŠµë‹ˆë‹¤.","ì „ì„¸ì‚¬ê¸°","ì´ˆê¸‰"
```

---

## ğŸš€ 7ë‹¨ê³„: ì‹¤í–‰ ìˆœì„œ

### 7.1 ë‹¨ê³„ë³„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```cmd
# 1. ê°€ìƒí™˜ê²½ í™œì„±í™”
llama_cpu_env\Scripts\activate

# 2. CPU ì •ë³´ í™•ì¸
run_cpu_scripts.bat check

# 3. ë§¤ìš° ì‘ì€ í…ŒìŠ¤íŠ¸ (10 ìŠ¤í…)
run_cpu_scripts.bat train_test

# 4. ê²°ê³¼ í™•ì¸
run_cpu_scripts.bat test

# 5. ì„±ê³µí•˜ë©´ ë” í° í›ˆë ¨
run_cpu_scripts.bat train_small
```

### 7.2 ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

```cmd
# ì‘ì—… ê´€ë¦¬ìì—ì„œ ëª¨ë‹ˆí„°ë§
# - CPU ì‚¬ìš©ë¥ : 90-100% ìœ ì§€ë˜ì–´ì•¼ í•¨
# - RAM ì‚¬ìš©ë¥ : 80% ì´í•˜ ìœ ì§€
# - ì˜¨ë„: CPU ì˜¨ë„ í™•ì¸ (ê³¼ì—´ ì£¼ì˜)
```

---

## ğŸ’¡ ì¶”ê°€ ìµœì í™” íŒ

### CPU ì„±ëŠ¥ ê·¹ëŒ€í™”
1. **BIOS ì„¤ì •**: Turbo Boost/Precision Boost í™œì„±í™”
2. **ì „ì› ê´€ë¦¬**: ê³ ì„±ëŠ¥ ëª¨ë“œ
3. **ë°±ê·¸ë¼ìš´ë“œ í”„ë¡œì„¸ìŠ¤**: ìµœì†Œí™”
4. **ì¿¨ë§**: CPU ì˜¨ë„ 65Â°C ì´í•˜ ìœ ì§€

### ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©
```python
# config_cpu.pyì—ì„œ ë” ì‘ì€ ëª¨ë¸ë¡œ ë³€ê²½
model_name: str = "microsoft/DialoGPT-small"  # 117M íŒŒë¼ë¯¸í„°
# ë˜ëŠ”
model_name: str = "distilgpt2"  # 82M íŒŒë¼ë¯¸í„°
```

### ì ì§„ì  í•™ìŠµ
```python
# ë§¤ìš° ì‘ì€ í•™ìŠµë¥ ê³¼ ì ì€ ìŠ¤í…ìœ¼ë¡œ ì‹œì‘
learning_rate: float = 1e-5
max_steps: int = 50
```

---

## âš¡ ì„±ëŠ¥ ì˜ˆìƒ

| ì„¤ì • | ì˜ˆìƒ ì‹œê°„ (i7 8ì„¸ëŒ€ ê¸°ì¤€) | ê¶Œì¥ ì‚¬ìš© |
|------|------------------------|-----------|
| 10 ìŠ¤í… í…ŒìŠ¤íŠ¸ | 5-10ë¶„ | ì´ˆê¸° í…ŒìŠ¤íŠ¸ |
| 100 ìŠ¤í… | 30-60ë¶„ | ê¸°ëŠ¥ í™•ì¸ |
| 1 ì—í­ (ì „ì²´) | 10-20ì‹œê°„ | ì‹¤ì œ í›ˆë ¨ |

CPU í›ˆë ¨ì€ ëŠë¦¬ì§€ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤! ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ ì‹œì‘í•´ì„œ ì ì§„ì ìœ¼ë¡œ í™•ì¥í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. ğŸ’ª